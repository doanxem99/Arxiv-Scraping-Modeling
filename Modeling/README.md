# ArXiv Reference Matching System

**Course:** NMKHDL - Lab 2  
**Student ID:** 23127088  
**Objective:** Automated matching of BibTeX entries with ArXiv metadata

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Environment Setup](#environment-setup)
3. [Project Structure](#project-structure)
4. [Installation Instructions](#installation-instructions)
5. [Execution Guide](#execution-guide)
6. [Output Files](#output-files)

---

## Project Overview

This system automatically matches BibTeX entries extracted from ArXiv LaTeX source files with their corresponding ArXiv metadata from `references.json` files. The pipeline uses hierarchical parsing, multi-stage data cleaning, feature engineering, and machine learning classification.

**Key Features:**
- Hierarchical LaTeX parsing (handles multi-file projects)
- Multi-tier data standardization (original â†’ cleaned â†’ no_stopwords)
- Feature engineering with 7 discriminative features
- Gradient Boosting classifier with hyperparameter optimization
- MRR@5 evaluation for ranking quality

**Performance:**
- **Parsing Success Rate:** 95.2%
- **Test MRR@5:** 0.8729
- **Perfect Match Rate:** 80% (rank 1)

---

## ðŸ› ï¸ Environment Setup

### Prerequisites

- **Operating System:** Linux (tested on Ubuntu 20.04+) or WSL2
- **Python Version:** 3.10 or higher
- **Memory:** At least 4GB RAM
- **Storage:** At least 2GB free space

### Required Python Packages

The following packages are required (see `requirements.txt` for exact versions):

```txt
# Core Scientific Computing
numpy>=1.24.0
pandas>=2.0.0
scipy>=1.10.0

# Machine Learning
scikit-learn>=1.3.0

# Data Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Jupyter Support (optional, for notebooks)
jupyter>=1.0.0
ipykernel>=6.25.0

# Text Processing (standard library, no install needed)
# - re, json, pathlib, unicodedata, difflib
```

---

## Project Structure

```
Lab2/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ report.md                          # Detailed implementation report
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ papers/                            # ArXiv source files (input)
â”‚   â”œâ”€â”€ 2312-15844/
â”‚   â”‚   â”œâ”€â”€ *.tex                      # LaTeX source files
â”‚   â”‚   â”œâ”€â”€ *.bib                      # Bibliography files (if any)
â”‚   â”‚   â”œâ”€â”€ metadata.json              # ArXiv metadata
â”‚   â”‚   â””â”€â”€ references.json            # ArXiv references
â”‚   â”œâ”€â”€ 2312-15845/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ bibtex/                            # Processed outputs (generated)
â”‚   â””â”€â”€ 23127088/                      # Student ID folder
â”‚       â”œâ”€â”€ 2312-15844/
â”‚       â”‚   â”œâ”€â”€ refs.bib               # Extracted BibTeX entries
â”‚       â”‚   â”œâ”€â”€ cleaned_data.json      # Cleaned and standardized data
â”‚       â”‚   â””â”€â”€ pred.json              # Model predictions (ranked list)
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ labels/                            # Ground truth labels (generated)
â”‚   â””â”€â”€ ground_truth_labels.json      # Manual + automatic labels
â”‚
â”œâ”€â”€ features/                          # Feature datasets (generated)
â”‚   â”œâ”€â”€ features_dataset.csv          # All samples with 7 features
â”‚   â”œâ”€â”€ features_dataset.json         # JSON format
â”‚   â””â”€â”€ feature_metadata.json         # Feature statistics
â”‚
â”œâ”€â”€ models/                            # Trained models and results (generated)
â”‚   â”œâ”€â”€ best_model.pkl                # Trained Gradient Boosting model
â”‚   â”œâ”€â”€ scaler.pkl                    # Feature scaler
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl          # TF-IDF vectorizer
â”‚   â”œâ”€â”€ model_metadata.json           # Model hyperparameters and metrics
â”‚   â”œâ”€â”€ validation_results.csv        # Validation predictions
â”‚   â”œâ”€â”€ test_predictions.csv          # Test predictions
â”‚   â”œâ”€â”€ test_evaluation_mrr.csv       # Detailed MRR results
â”‚   â”œâ”€â”€ confusion_matrix.png          # Confusion matrix plot
â”‚   â”œâ”€â”€ roc_curve.png                 # ROC curve plot
â”‚   â”œâ”€â”€ feature_importance_model.png  # Feature importance plot
â”‚   â”œâ”€â”€ mrr_evaluation.png            # MRR comparison plot
â”‚   â””â”€â”€ rank_distribution_test.png    # Rank distribution plot
â”‚
â””â”€â”€ src/                               # Source code (Jupyter notebooks)
    â”œâ”€â”€ 2_1_Data_Cleaning.ipynb        # Step 1: Parse and clean data
    â”œâ”€â”€ 2_2_Data_Labelling.ipynb       # Step 2: Create ground truth labels
    â”œâ”€â”€ 2_3_Feature_Engineer.ipynb     # Step 3: Extract features
    â”œâ”€â”€ 2_4_Data_Modeling-Evaluation.ipynb  # Step 4: Train and evaluate model
    â””â”€â”€ 3_3_Constructt_Submission_Folder.ipynb  # Step 5: Prepare submission
```

---

## Installation Instructions

### Step 1: Clone or Extract Project

```bash
# If using git
git clone <repository_url>
cd Lab2

# Or extract from archive
unzip Lab2.zip
cd Lab2
```

### Step 2: Create Python Virtual Environment (Recommended)

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# On Linux/Mac:
source venv/bin/activate

# On Windows (WSL2):
source venv/bin/activate
```

or using conda:

```bash
conda create -n lab2_env python=3.10 -y
conda activate lab2_env
```

### Step 3: Install Dependencies

```bash
# Upgrade pip first
pip install --upgrade pip

# Install required packages
pip install -r requirements.txt

# Verify installation
python -c "import sklearn, pandas, numpy, matplotlib; print('All packages installed successfully')"
```

### Step 4: Verify Data Structure

Ensure the `papers/` directory contains ArXiv source files:

```bash
# Check if papers directory exists
ls -la papers/

# Should see directories like: 2312-15844, 2312-15845, etc.
# Each directory should contain: *.tex, references.json, metadata.json
```

---

## Execution Guide

### Quick Start (Run Full Pipeline)

Execute all notebooks in sequence:

```bash
# Navigate to src directory
cd src

# Run notebooks in order (if using command line)
jupyter nbconvert --execute --to notebook \
    --inplace 2_1_Data_Cleaning.ipynb \
    --ExecutePreprocessor.timeout=600

jupyter nbconvert --execute --to notebook \
    --inplace 2_2_Data_Labelling.ipynb \
    --ExecutePreprocessor.timeout=600

jupyter nbconvert --execute --to notebook \
    --inplace 2_3_Feature_Engineer.ipynb \
    --ExecutePreprocessor.timeout=600

jupyter nbconvert --execute --to notebook \
    --inplace 2_4_Data_Modeling-Evaluation.ipynb \
    --ExecutePreprocessor.timeout=600
```

**Or use Jupyter Notebook/Lab interface:**

```bash
# Start Jupyter
jupyter notebook

# Or Jupyter Lab
jupyter lab

# Then open and run each notebook in order
```

---

### Step-by-Step Execution

#### **Step 1: Data Cleaning and Parsing**

**Notebook:** `2_1_Data_Cleaning.ipynb`

**Purpose:** 
- Parse BibTeX entries from LaTeX source files
- Clean and standardize text (titles, authors, years)
- Create hierarchical representations

**Execution:**

```bash
# Open notebook
jupyter notebook 2_1_Data_Cleaning.ipynb

# Run all cells: Cell â†’ Run All
# Or use: Shift+Enter on each cell
```

**Expected Runtime:** ~2-3 minutes

**Output:**
- `bibtex/23127088/<paper_id>/refs.bib` - Extracted BibTeX entries
- `bibtex/23127088/<paper_id>/cleaned_data.json` - Cleaned data with hierarchies

**Success Indicators:**
```
âœ“ Loaded X papers
âœ“ Processed X BibTeX entries
âœ“ Cleaning complete
âœ“ Saved cleaned data for X publications
```

---

#### **Step 2: Ground Truth Labeling**

**Notebook:** `2_2_Data_Labelling.ipynb`

**Purpose:**
- Create ground truth labels (manual + automatic)
- Match BibTeX entries to ArXiv IDs

**Execution:**

```bash
jupyter notebook 2_2_Data_Labelling.ipynb
# Run all cells
```

**Expected Runtime:** ~1 minute

**Output:**
- `labels/ground_truth_labels.json` - Manual and automatic labels

**Success Indicators:**
```
âœ“ Manual labels: 158
âœ“ Automatic labels: 0
âœ“ Total ground truth: 158
âœ“ Saved labels to labels/ground_truth_labels.json
```

---

#### **Step 3: Feature Engineering**

**Notebook:** `2_3_Feature_Engineer.ipynb`

**Purpose:**
- Generate (BibTeX, ArXiv) pairs
- Extract 7 features per pair
- Create balanced training dataset

**Execution:**

```bash
jupyter notebook 2_3_Feature_Engineer.ipynb
# Run all cells
```

**Expected Runtime:** ~1-2 minutes

**Output:**
- `features/features_dataset.csv` - Feature dataset (338 samples Ã— 7 features)
- `features/features_dataset.json` - JSON format
- `features/feature_metadata.json` - Feature statistics

**Success Indicators:**
```
âœ“ Created 158 positive samples
âœ“ Created 180 negative samples
âœ“ Total samples: 338
âœ“ Features: 7
âœ“ Saved to features/features_dataset.csv
```

---

#### **Step 4: Model Training and Evaluation**

**Notebook:** `2_4_Data_Modeling-Evaluation.ipynb`

**Purpose:**
- Split data into train/valid/test sets
- Train Gradient Boosting classifier with Grid Search
- Evaluate using MRR@5 metric
- Generate predictions for all publications

**Execution:**

```bash
jupyter notebook 2_4_Data_Modeling-Evaluation.ipynb
# Run all cells
```

**Expected Runtime:** ~3-5 minutes (Grid Search may take longer)

**Output:**
- `models/best_model.pkl` - Trained model
- `models/scaler.pkl` - Feature scaler
- `models/tfidf_vectorizer.pkl` - TF-IDF vectorizer
- `models/model_metadata.json` - Hyperparameters and metrics
- `models/test_evaluation_mrr.csv` - Test results
- `bibtex/23127088/<paper_id>/pred.json` - Predictions for each paper

**Success Indicators:**
```
âœ“ Grid Search complete
âœ“ Best MRR@5 (Test): 0.8729
âœ“ Generated pred.json for 166 publications
âœ“ Perfect matches (Rank 1): 80%
âœ“ All models saved to models/
```

---

#### **Step 5: Construct Submission Folder** (Optional)

**Notebook:** `3_3_Constructt_Submission_Folder.ipynb`

**Purpose:**
- Copy metadata.json and references.json to submission folder
- Prepare final submission structure

**Execution:**

```bash
jupyter notebook 3_3_Constructt_Submission_Folder.ipynb
# Run all cells
```

**Expected Runtime:** <1 minute

**Output:**
- Copies files to `bibtex/23127088/<paper_id>/` for each publication

---

## Output Files

### Key Output Files

| File | Description | Usage |
|------|-------------|-------|
| `bibtex/23127088/<paper_id>/pred.json` | **Ranked predictions** for each BibTeX entry | Final output for submission |
| `models/model_metadata.json` | Model hyperparameters and performance | Reproducibility |
| `features/features_dataset.csv` | Feature dataset with labels | Analysis |
| `labels/ground_truth_labels.json` | Ground truth labels | Training/evaluation |

### Understanding pred.json

Each `pred.json` file contains:

```json
{
  "partition": "test",
  "groundtruth": {
    "smith2020": "2001.12345"
  },
  "prediction": {
    "smith2020": [
      "2001.12345",   // Rank 1 (correct match)
      "2002.54321",   // Rank 2
      "2003.98765",   // Rank 3
      "...",
      "..."
    ]
  }
}
```

- **partition:** train/valid/test
- **groundtruth:** BibTeX key â†’ correct ArXiv ID
- **prediction:** BibTeX key â†’ ranked list of ArXiv IDs (sorted by match probability)

---

### Performance Optimization

If execution is slow:

1. **Use fewer papers for testing:**
   ```python
   # In 2_1_Data_Cleaning.ipynb
   papers = papers[:10]  # Process only first 10 papers
   ```

2. **Reduce negative samples:**
   ```python
   # In 2_3_Feature_Engineer.ipynb
   negative_ratio = 1.0  # Instead of 2.0
   ```

3. **Skip Grid Search (use default parameters):**
   ```python
   # In 2_4_Data_Modeling-Evaluation.ipynb
   best_model = GradientBoostingClassifier(
       n_estimators=50, 
       max_depth=3, 
       learning_rate=0.1, 
       random_state=42
   )
   best_model.fit(X_train_scaled, y_train)
   ```

---

### Generated Files

```bash
# Check output files
ls -lh models/
# Should see: best_model.pkl, model_metadata.json, *.png, *.csv

ls -lh features/
# Should see: features_dataset.csv, feature_metadata.json

ls -lh bibtex/23127088/2312-15846/
# Should see: refs.bib, cleaned_data.json, pred.json
```